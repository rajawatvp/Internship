{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87a31a8f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eacf2af6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import all the possible libraries which all needed. \n",
    "\n",
    "import selenium\n",
    "import requests\n",
    "import pandas as pd\n",
    "import urllib.request\n",
    "import re\n",
    "import os \n",
    "from tqdm import tqdm\n",
    "from selenium import webdriver\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from selenium.webdriver.common.by import By\n",
    "import time\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from time import sleep\n",
    "from selenium.common.exceptions import ElementNotInteractableException\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from selenium.common.exceptions import StaleElementReferenceException\n",
    "from selenium.webdriver.chrome.options import Options"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5478d1dd",
   "metadata": {},
   "source": [
    "### 1. Write a python program which searches all the product under a particular product from `www.amazon.in`. The product to be searched will be taken as input from user. For e.g. If user input is ‘guitar’. Then search for guitars."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d43ad00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the access from chrome driver\n",
    "\n",
    "driver = webdriver.Chrome()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "430d3e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "#website url\n",
    "\n",
    "driver.get(\"https://www.amazon.in/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa37a49a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#search bar selection\n",
    "\n",
    "search_bar=driver.find_element(By.ID,\"twotabsearchtextbox\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec73a734",
   "metadata": {},
   "outputs": [],
   "source": [
    "#take input from user: Note press enter after entering the input to get out of loop\n",
    "\n",
    "print(\"Search Product you like \")\n",
    "search_for=input()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e1d1363",
   "metadata": {},
   "outputs": [],
   "source": [
    "#search the input \n",
    "\n",
    "search_bar.clear()\n",
    "search_bar.send_keys(search_for)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd31994c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#click on search button icon\n",
    "\n",
    "search_button=driver.find_element(By.ID,\"nav-search-submit-button\")\n",
    "search_button.click()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5029de9",
   "metadata": {},
   "source": [
    "###  2. scrape the following details of each product listed in first 3 pages of your search results and save it in a dataframe and csv. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6b021e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#fetch the urls of all products \n",
    "\n",
    "start_page = 0\n",
    "end_page = 2\n",
    "urls = []\n",
    "for page in range(start_page,end_page+1):\n",
    "    try:\n",
    "        page_urls = driver.find_elements(By.XPATH,'//a[@class=\"a-link-normal s-no-outline\"]')\n",
    "        \n",
    "        # appending all the urls on current page to urls list\n",
    "        for url in page_urls:\n",
    "            url = url.get_attribute('href')     \n",
    "            if url[0:4]=='http':                # Checking if the scraped data is a valid url or not\n",
    "                urls.append(url)                \n",
    "        print(\"Product urls of page {} has been scraped.\".format(page+1))\n",
    "        \n",
    "        # Moving to next page\n",
    "        nxt_button = driver.find_element(By.XPATH,'//a[@class=\"s-pagination-item s-pagination-next s-pagination-button s-pagination-separator\"]')      # Locating the next_button which is active\n",
    "        if nxt_button.text == 'Next→':                                            # Checking if the button located is next button\n",
    "            nxt_button.click()                                                    \n",
    "            time.sleep(5)                                                         \n",
    "            \n",
    "        # If the current active button is not next button, the we will check if the next button is inactive or not    \n",
    "        elif driver.find_element(By.XPATH,'//a[@class=\"s-pagination-item s-pagination-next s-pagination-button s-pagination-separator\"]').text == 'Next→':    \n",
    "            print(\"No new page Found so, Breaking the loop\")  \n",
    "            break\n",
    "            \n",
    "    except StaleElementReferenceException as e:             # Handling StaleElement Exception   \n",
    "        print(\"Stale Exception\")\n",
    "        next_page = nxt_button.get_attribute('href')        \n",
    "        driver.get(next_page)                               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99f794cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#check if all products from 1st 3 pages are scraped\n",
    "len(urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d47c7d2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## create the list for store the data\n",
    "brand_list=[]\n",
    "title_list=[]\n",
    "rating_list=[]\n",
    "no_rating_list=[]\n",
    "price_list=[]\n",
    "return_list=[]\n",
    "expected_delivery_list=[]\n",
    "availability_list=[]\n",
    "details_list=[]\n",
    "img_url_list=[]\n",
    "     \n",
    "#go to each items url to fetch data   \n",
    "for a in urls:\n",
    "    driver.get(a)\n",
    "    \n",
    "    # brand\n",
    "    try:\n",
    "        brand=driver.find_element(By.XPATH,\"//table[@class='a-keyvalue prodDetTable']/tbody/tr[1]/td\")\n",
    "        brand_list.append(brand.text)\n",
    "    except NoSuchElementException as e:\n",
    "        brand_list.append(\"--\")\n",
    "    \n",
    "    # title \n",
    "    try:\n",
    "        title=driver.find_element(By.XPATH,\"//h1[@class='a-size-large a-spacing-none']\")\n",
    "        title_list.append(title.text)\n",
    "    except NoSuchElementException as e:\n",
    "        title_list.append(\"--\")\n",
    "        \n",
    "    # ratings\n",
    "    try:\n",
    "        rating=driver.find_element(By.XPATH,\"//*[@id='productDetails_detailBullets_sections1']/tbody/tr[2]/td\")\n",
    "        rating_list.append(rating.text.split(\"\\n\")[-1])\n",
    "    except NoSuchElementException as e:\n",
    "        rating_list.append(\"--\")\n",
    "        \n",
    "    # number of ratings\n",
    "    try:\n",
    "        no_rating=driver.find_element(By.ID,\"averageCustomerReviews\")\n",
    "        no_rating_list.append(no_rating.text)\n",
    "    except NoSuchElementException as e:\n",
    "        no_rating_list.append(\"--\")\n",
    "        \n",
    "    # price\n",
    "    try:\n",
    "        price=driver.find_element(By.XPATH,\"//table[@class='a-lineitem']/tbody/tr[2]/td[2]/span\")\n",
    "        price_list.append(price.text)\n",
    "    except NoSuchElementException as e:\n",
    "        price_list.append(\"--\")  \n",
    "        \n",
    "    # return\n",
    "    try:\n",
    "        returns=driver.find_element(By.XPATH,\"//div[@class='a-row icon-farm-wrapper']/div[2]/span/div[2]/a\")\n",
    "        return_list.append(returns.text)\n",
    "    except NoSuchElementException as e:\n",
    "        return_list.append(\"--\")\n",
    "        \n",
    "    # expected_delivery\n",
    "    try:\n",
    "        expected_delivery=driver.find_element(By.XPATH,\"//span[@class='a-color-base a-text-bold']\")\n",
    "        expected_delivery_list.append(expected_delivery.text)\n",
    "    except NoSuchElementException as e:\n",
    "        expected_delivery_list.append(\"--\")\n",
    "    \n",
    "        \n",
    "    # availability\n",
    "    try:\n",
    "        availability=driver.find_element(By.XPATH,\"//span[@class='a-size-medium a-color-success']\")\n",
    "        availability_list.append(availability.text)\n",
    "    except NoSuchElementException as e:\n",
    "        availability_list.append(\"--\")   \n",
    "        \n",
    "    #other details\n",
    "    try:\n",
    "        details=driver.find_element(By.XPATH,\"//div[@class='a-row a-spacing-top-base']\")\n",
    "        details_list.append(details.text.replace(\"\\n\",\" \"))\n",
    "    except NoSuchElementException as e:\n",
    "        details_list.append(\"--\")\n",
    "        \n",
    "    #product image url\n",
    "    try:\n",
    "        img_url=driver.find_element(By.XPATH,\"//div[@class='imgTagWrapper']/img\")\n",
    "        img_url_list.append(img_url.get_attribute(\"src\"))\n",
    "    except NoSuchElementException as e:\n",
    "        img_url_list.append(\"--\")\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55cde653",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(brand_list[0:2])\n",
    "print(title_list[0:2])\n",
    "print(rating_list[0:2])\n",
    "print(no_rating_list[0:2])\n",
    "print(price_list[0:2])\n",
    "print(return_list[0:2])\n",
    "print(expected_delivery_list[0:2])\n",
    "print(availability_list[0:2])\n",
    "print(details_list[0:1])\n",
    "print(img_url_list[0:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "899b49d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#check if the length of all lists are equal\n",
    "print(len(brand_list))\n",
    "print(len(title_list))\n",
    "print(len(rating_list))\n",
    "print(len(no_rating_list))\n",
    "print(len(price_list))\n",
    "print(len(return_list))\n",
    "print(len(expected_delivery_list))\n",
    "print(len(availability_list))\n",
    "print(len(details_list))\n",
    "print(len(img_url_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5e4507d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating data frame\n",
    "df=pd.DataFrame({})\n",
    "df['Brand']=brand_list\n",
    "df['Name of the product']=title_list\n",
    "df[\"Ratings\"]=rating_list\n",
    "df[\"No of ratings\"]=no_rating_list\n",
    "df['Price']=price_list\n",
    "df['Return/Exchange']=return_list\n",
    "df['Expected Delivery']=expected_delivery_list\n",
    "df['Availability']=availability_list\n",
    "df['Other details']=details_list\n",
    "df['Product Urls']=img_url_list\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5164080d",
   "metadata": {},
   "source": [
    "### 3. Write a python program to access the search bar and search button on images.google.com and scrape 10 images each for keywords ‘fruits’, ‘cars’ and ‘Machine Learning’, ‘Guitar’, ‘Cakes’."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db7620a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#connect to driver\n",
    "driver=webdriver.Chrome()\n",
    "driver.get('https://images.google.com/')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae0e3f93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding the search bar using it's xpath\n",
    "\n",
    "search_bar = driver.find_element(By.ID,\"APjFqb\")    \n",
    "\n",
    "# Inputing \"banana\" keyword to search rock images\n",
    "search_bar.send_keys(\"fruits\")       \n",
    "\n",
    "# Finding the xpath of search button\n",
    "search_button = driver.find_element(By.XPATH,\"/html/body/div[1]/div[3]/form/div[1]/div[1]/div[1]/button\")    \n",
    "\n",
    " # Clicking the search button\n",
    "search_button.click()       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "728552f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 50 time we scroll down by 100 in order to generate more images on the website\n",
    "\n",
    "driver.execute_script(\"window.scrollBy(0,100)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5af8dcf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Fetch images by using XPATH\n",
    "\n",
    "images = driver.find_elements(By.XPATH,'//div[@class=\"fR600b islir\"]/img')\n",
    "\n",
    "img_urls = []\n",
    "img_data = []\n",
    "for image in images:\n",
    "    source= image.get_attribute('src')\n",
    "    if source is not None:\n",
    "        if(source[0:4] == 'http'):\n",
    "            img_urls.append(source)\n",
    "len(img_urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e804edf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "## using for loop for save the continue image data in the system folder\n",
    "\n",
    "for i in range(len(img_urls)):\n",
    "    if i >= 100:\n",
    "        break\n",
    "    print(\"Downloading {0} of {1} images\" .format(i, 100))\n",
    "    response= requests.get(img_urls[i])\n",
    "    file = open(\"D:\\Internship _Projects_DATA\\images\"+str(i)+\".jpg\", \"wb\")\n",
    "    file.write(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b444771",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding Search bar website by class\n",
    "Search=driver.find_element(By.XPATH,'//div[@class=\"dGWkFc\"]/input')\n",
    "\n",
    "# Clearing any previous input in search bar\n",
    "Search.clear()\n",
    "\n",
    "# Feeding input 'Cars' in search bar\n",
    "Search.send_keys('Cars')\n",
    "\n",
    "# Finding Search button for clicking through class name\n",
    "Search_button=driver.find_element(By.CLASS_NAME,'rCGXm')\n",
    "\n",
    "# Clicking search button\n",
    "Search_button.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaaf8c2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 50 time we scroll down by 200 in order to generate more images on the website\n",
    "\n",
    "driver.execute_script(\"window.scrollBy(0,200)\",\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2302068",
   "metadata": {},
   "outputs": [],
   "source": [
    "URL=[]\n",
    "images= driver.find_elements(By.XPATH,'//div[@class=\"fR600b islir\"]/img')\n",
    "for image in images:\n",
    "    source= image.get_attribute('src')\n",
    "    if source is not None:\n",
    "        if(source[0:4] == 'http'):\n",
    "            URL.append(source)\n",
    "            \n",
    "len(URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52b4716c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## using for loop for save the continue image data in the system folder\n",
    "\n",
    "images= driver.find_elements(By.XPATH,'//div[@class=\"fR600b islir\"]/img')\n",
    "\n",
    "img_urls = []\n",
    "img_data = []\n",
    "for image in images:\n",
    "    source= image.get_attribute('src')\n",
    "    if source is not None:\n",
    "        if(source[0:4] == 'http'):\n",
    "            img_urls.append(source)\n",
    "            \n",
    "for i in range(len(img_urls)):\n",
    "    if i >= 100:\n",
    "        break\n",
    "    print(\"Downloading {0} of {1} images\" .format(i, 100))\n",
    "    response= requests.get(img_urls[i])\n",
    "    file = open(\"D:\\Internship _Projects_DATA\\images\" +str(i)+ \".jpg\", \"wb\")\n",
    "    file.write(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f059348",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding Search bar website by class\n",
    "Search=driver.find_element(By.XPATH,'//div[@class=\"dGWkFc\"]/input')\n",
    "\n",
    "# Clearing any previous input in search bar\n",
    "Search.clear()\n",
    "\n",
    "# Feeding input 'Cars' in search bar\n",
    "Search.send_keys('Machine Learning')\n",
    "\n",
    "# Finding Search button for clicking through class name\n",
    "Search_button=driver.find_element(By.CLASS_NAME,'rCGXm')\n",
    "\n",
    "# Clicking search button\n",
    "Search_button.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b04f6d93",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.execute_script(\"window.scrollBy(0,100)\",\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3885114",
   "metadata": {},
   "outputs": [],
   "source": [
    "URL=[]\n",
    "images= driver.find_elements(By.XPATH,'//div[@class=\"fR600b islir\"]/img')\n",
    "for image in images:\n",
    "    source= image.get_attribute('src')\n",
    "    if source is not None:\n",
    "        if(source[0:4] == 'http'):\n",
    "            URL.append(source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba8ca02b",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b25de09",
   "metadata": {},
   "outputs": [],
   "source": [
    "## using for loop for save the continue image data in the system folder\n",
    "\n",
    "images= driver.find_elements(By.XPATH,'//div[@class=\"fR600b islir\"]/img')\n",
    "\n",
    "img_urls = []\n",
    "img_data = []\n",
    "for image in images:\n",
    "    source= image.get_attribute('src')\n",
    "    if source is not None:\n",
    "        if(source[0:4] == 'http'):\n",
    "            img_urls.append(source)\n",
    "            \n",
    "for i in range(len(img_urls)):\n",
    "    if i >= 100:\n",
    "        break\n",
    "    print(\"Downloading {0} of {1} images\" .format(i, 100))\n",
    "    response= requests.get(img_urls[i])\n",
    "    file = open(\"D:\\Internship _Projects_DATA\\images\" +str(i)+ \".jpg\", \"wb\")\n",
    "    file.write(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7978eb7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding Search bar website by class\n",
    "Search=driver.find_element(By.XPATH,'//div[@class=\"dGWkFc\"]/input')\n",
    "\n",
    "# Clearing any previous input in search bar\n",
    "Search.clear()\n",
    "\n",
    "# Feeding input 'Cars' in search bar\n",
    "Search.send_keys('Guitars')\n",
    "\n",
    "# Finding Search button for clicking through class name\n",
    "Search_button=driver.find_element(By.CLASS_NAME,'rCGXm')\n",
    "\n",
    "# Clicking search button\n",
    "Search_button.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3049f33d",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.execute_script(\"window.scrollBy(0,100)\",\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c054b9ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "URL=[]\n",
    "images= driver.find_elements(By.XPATH,'//div[@class=\"fR600b islir\"]/img')\n",
    "for image in images:\n",
    "    source= image.get_attribute('src')\n",
    "    if source is not None:\n",
    "        if(source[0:4] == 'http'):\n",
    "            URL.append(source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f184b35c",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2078d83",
   "metadata": {},
   "outputs": [],
   "source": [
    "## using for loop for save the continue image data in the system folder\n",
    "\n",
    "images= driver.find_elements(By.XPATH,'//div[@class=\"fR600b islir\"]/img')\n",
    "\n",
    "img_urls = []\n",
    "img_data = []\n",
    "for image in images:\n",
    "    source= image.get_attribute('src')\n",
    "    if source is not None:\n",
    "        if(source[0:4] == 'http'):\n",
    "            img_urls.append(source)\n",
    "            \n",
    "for i in range(len(img_urls)):\n",
    "    if i >= 100:\n",
    "        break\n",
    "    print(\"Downloading {0} of {1} images\" .format(i, 100))\n",
    "    response= requests.get(img_urls[i])\n",
    "    file = open(\"D:\\Internship _Projects_DATA\\images\" +str(i)+ \".jpg\", \"wb\")\n",
    "    file.write(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8837b8ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding Search bar website by class\n",
    "Search=driver.find_element(By.XPATH,'//div[@class=\"dGWkFc\"]/input')\n",
    "\n",
    "# Clearing any previous input in search bar\n",
    "Search.clear()\n",
    "\n",
    "# Feeding input 'Cars' in search bar\n",
    "Search.send_keys('Cakes')\n",
    "\n",
    "# Finding Search button for clicking through class name\n",
    "Search_button=driver.find_element(By.CLASS_NAME,'rCGXm')\n",
    "\n",
    "# Clicking search button\n",
    "Search_button.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e363e267",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.execute_script(\"window.scrollBy(0,100)\",\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5860591",
   "metadata": {},
   "outputs": [],
   "source": [
    "URL=[]\n",
    "images= driver.find_elements(By.XPATH,'//div[@class=\"fR600b islir\"]/img')\n",
    "for image in images:\n",
    "    source= image.get_attribute('src')\n",
    "    if source is not None:\n",
    "        if(source[0:4] == 'http'):\n",
    "            URL.append(source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15f0170a",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8d7c807",
   "metadata": {},
   "outputs": [],
   "source": [
    "## using for loop for save the continue image data in the system folder\n",
    "\n",
    "images= driver.find_elements(By.XPATH,'//div[@class=\"fR600b islir\"]/img')\n",
    "\n",
    "img_urls = []\n",
    "img_data = []\n",
    "for image in images:\n",
    "    source= image.get_attribute('src')\n",
    "    if source is not None:\n",
    "        if(source[0:4] == 'http'):\n",
    "            img_urls.append(source)\n",
    "            \n",
    "for i in range(len(img_urls)):\n",
    "    if i >= 100:\n",
    "        break\n",
    "    print(\"Downloading {0} of {1} images\" .format(i, 100))\n",
    "    response= requests.get(img_urls[i])\n",
    "    file = open(\"D:\\Internship _Projects_DATA\\images\" +str(i)+ \".jpg\", \"wb\")\n",
    "    file.write(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e18f0d5",
   "metadata": {},
   "source": [
    "### 4. Write a python program to search for a smartphone(e.g.: Oneplus Nord, pixel 4A, etc.) on `www.flipkart.com`\n",
    "\n",
    "#### and scrape following details for all the search results displayed on 1st page. Details to be scraped: “Brand Name”, “Smartphone name”, “Colour”, “RAM”, “Storage(ROM)”, “Primary Camera”,“Secondary Camera”, “Display Size”, “Battery Capacity”, “Price”, “Product URL”. Incase if any of the details is missing then replace it by “- “. Save your results in a dataframe and CSV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef0fd63a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#connect to driver\n",
    "driver=webdriver.Chrome()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ee31ca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "## get to the flipkart webpage\n",
    "\n",
    "url4=\"https://www.flipkart.com/search?q=smartphone&otracker=search&otracker1=search&marketplace=FLIPKART&as-show=on&as=off\"\n",
    "driver.get(url4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ccddb53",
   "metadata": {},
   "outputs": [],
   "source": [
    "## ## create the list for store the data\n",
    "\n",
    "Brand_Name=[]\n",
    "Colour=[]                    ## Color is not available seprately it is with model and phone name\n",
    "Storage_RAM_ROM=[]\n",
    "P_F_Camera=[]\n",
    "Display_size_Resolution=[]\n",
    "Battery=[]\n",
    "Price=[]\n",
    "Product_URL=[]               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aec4988",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Brand name\n",
    "\n",
    "BName=driver.find_elements(By.XPATH,\"//div[@class='KzDlHZ']\")\n",
    "for i in BName:\n",
    "    if i.text is None :\n",
    "        Brand_Name.append(\"--\") \n",
    "    else:\n",
    "        Brand_Name.append(i.text)\n",
    "print(len(Brand_Name),Brand_Name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07e52deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "## RAM and ROM Storage\n",
    "\n",
    "ram=driver.find_elements(By.XPATH,\"//ul[@class='G4BRas']/li[1]\")\n",
    "for i in ram:\n",
    "    if i.text is None :\n",
    "        Storage_RAM_ROM.append(\"--\") \n",
    "    else:\n",
    "        Storage_RAM_ROM.append(i.text.split())\n",
    "print(len(Storage_RAM_ROM),Storage_RAM_ROM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c99d5996",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Primary and Secodry Camera\n",
    "\n",
    "PC=driver.find_elements(By.XPATH,\"//ul[@class='G4BRas']/li[3]\")\n",
    "for i in PC:\n",
    "    if i.text is None :\n",
    "        P_F_Camera.append(\"--\") \n",
    "    else:\n",
    "        P_F_Camera.append(i.text)\n",
    "print(len(P_F_Camera),P_F_Camera)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a3c19c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Display size\n",
    "\n",
    "DS=driver.find_elements(By.XPATH,\"//ul[@class='G4BRas']/li[2]\")\n",
    "for i in DS:\n",
    "    if i.text is None :\n",
    "        Display_size_Resolution.append(\"--\") \n",
    "    else:\n",
    "        Display_size_Resolution.append(i.text)\n",
    "print(len(Display_size_Resolution),Display_size_Resolution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fca3e6f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Battery Capacity\n",
    "\n",
    "B=driver.find_elements(By.XPATH,\"//ul[@class='G4BRas']//li[4]\")\n",
    "for i in B:\n",
    "    if i.text is None :\n",
    "        Battery.append(\"--\") \n",
    "    else:\n",
    "        Battery.append(i.text)\n",
    "print(len(Battery),Battery)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8618e9b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Price\n",
    "\n",
    "price=driver.find_elements(By.XPATH,\"//div[@class='Nx9bqj _4b5DiR']\")\n",
    "for i in price:\n",
    "    if i.text is None :\n",
    "        Price.append(\"--\") \n",
    "    else:\n",
    "        Price.append(i.text)\n",
    "print(len(Price),Price)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6f64c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Product URL\n",
    "\n",
    "product_url=driver.find_elements(By.XPATH,\"//div[@class='tUxRFH']/a\")\n",
    "for i in product_url:\n",
    "    if i.text is None :\n",
    "        Product_URL.append(\"--\") \n",
    "    else:\n",
    "        Product_URL.append(i.text)\n",
    "print(len(Product_URL),Product_URL)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e459a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create DataFrame\n",
    "\n",
    "df=pd.DataFrame([])\n",
    "df['Brand_Name & Phone_Name']=Brand_Name[0:24]\n",
    "df['RAM & ROM Storage']=Storage_RAM_ROM[0:24]\n",
    "df['Camera']=P_F_Camera[0:24]\n",
    "df['Display_size']=Display_size_Resolution[0:24]\n",
    "df['Battery_Capacity']=Battery[0:24]\n",
    "df['Price']=Price[0:24]\n",
    "df['Product_url']=Product_url[0:24]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a37a6db0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30721635",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Convert the dataframe to CSV file\n",
    "\n",
    "df.to_csv(\"flipkart_smartphones_data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "651062bb",
   "metadata": {},
   "source": [
    "### Q.5 ) Write a program to scrap geospatial coordinates (latitude, longitude) of a city searched on google maps.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b98ca23",
   "metadata": {},
   "outputs": [],
   "source": [
    "#connect to driver\n",
    "driver=webdriver.Chrome()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23c6e8ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# opening google maps\n",
    "driver.get(\"https://www.google.co.in/maps\")\n",
    "time.sleep(3)\n",
    "\n",
    "# Enter city to be searched\n",
    "city = input('Enter City Name : ')\n",
    "\n",
    "# locating search bar\n",
    "search = driver.find_element(By.ID,\"searchboxinput\")  \n",
    "\n",
    " # clearing search bar\n",
    "search.clear()                                                            \n",
    "time.sleep(2)\n",
    "\n",
    "# entering values in search bar\n",
    "search.send_keys(city)              \n",
    "\n",
    "# locating search button\n",
    "button = driver.find_element(By.ID,\"searchbox-searchbutton\")\n",
    "\n",
    "# clicking search button\n",
    "button.click()                                                             \n",
    "time.sleep(3)\n",
    "\n",
    "try:\n",
    "    url_string = driver.current_url\n",
    "    print(\"URL Extracted: \", url_string)\n",
    "    lat_lng = re.findall(r'@(.*)data',url_string)\n",
    "    if len(lat_lng):\n",
    "        lat_lng_list = lat_lng[0].split(\",\")\n",
    "        if len(lat_lng_list)>=2:\n",
    "            lat = lat_lng_list[0]\n",
    "            lng = lat_lng_list[1]\n",
    "        print(\"Latitude = {}, Longitude = {}\".format(lat, lng))\n",
    "\n",
    "except Exception as e:\n",
    "        print(\"Error: \", str(e))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df526abe",
   "metadata": {},
   "source": [
    "### 6. Write a program to scrap all the available details of best gaming laptops from `digit.in`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "330a101e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#connect to driver\n",
    "driver=webdriver.Chrome()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da16af71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visit the web page\n",
    "url=\"https://www.digit.in/top-products/best-gaming-laptops-40.html\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18b951e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the webpage\n",
    "\n",
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77fa59cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "## create the list for store the data\n",
    "\n",
    "Brands=[]\n",
    "Products_Description=[]\n",
    "Specification=[]\n",
    "Price=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4b455e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## brand\n",
    "br=driver.find_elements(By.XPATH,\"//div[@class='rh_gr_top_middle mb10 colored_rate_bar']/h3/a\")\n",
    "len(br)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58dd286d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## run for loop for getting brand name data\n",
    "\n",
    "for i in br:\n",
    "   \n",
    "    Brands.append(str(i.text).replace(\"\\n\",\"\"))\n",
    "Brands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7afc287",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Discription\n",
    "\n",
    "discription=driver.find_elements(By.XPATH,\"//div[@class='woo_code_zone_loop clearbox']\")\n",
    "len(discription)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13974caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "## run for loop for getting discription data\n",
    "\n",
    "for i in discription:\n",
    "   \n",
    "    Specification.append(str(i.text).replace(\"\\n\",\"\"))\n",
    "Specification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c264950",
   "metadata": {},
   "outputs": [],
   "source": [
    "## product\n",
    "\n",
    "prod_disc=driver.find_elements(By.TAG_NAME,\"p\")\n",
    "len(prod_disc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70b133be",
   "metadata": {},
   "outputs": [],
   "source": [
    "## run for loop for getting Product discription data\n",
    "\n",
    "for i in prod_disc:\n",
    "   \n",
    "    Products_Description.append(str(i.text).replace(\"\\n\",\" \"))\n",
    "Products_Description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9e655c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "## price\n",
    "\n",
    "pri=driver.find_elements(By.XPATH,\"//div[@class='cegg-price-row']\")\n",
    "len(pri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a511c19a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## run for loop for getting price data\n",
    "\n",
    "for i in pri:\n",
    "   \n",
    "    Price.append(str(i.text).replace(\"\\n\",\"\"))\n",
    "Price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e802a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "## create the list for store the data\n",
    "\n",
    "digit_lap=pd.DataFrame([])\n",
    "digit_lap['Brands']=Brands[0:10]\n",
    "digit_lap['Description']=Products_Description[0:10]\n",
    "digit_lap['Specification']=Specification[0:10]\n",
    "digit_lap['Price']=Price[0:10]\n",
    "\n",
    "\n",
    "digit_lap"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "289ea81c",
   "metadata": {},
   "source": [
    "### 7. Write a python program to scrape the details for all billionaires from `www.forbes.com`. Details to be scrapped: “Rank”, “Name”, “Net worth”, “Age”, “Citizenship”, “Source”, “Industry”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd182d48",
   "metadata": {},
   "outputs": [],
   "source": [
    "## get the Chrome Access\n",
    "\n",
    "driver = webdriver.Chrome()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3de0970",
   "metadata": {},
   "outputs": [],
   "source": [
    "## take the URL of the Website\n",
    "\n",
    "url = \"https://www.forbes.com/billionaires/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e98951d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## get the webpage access through URL\n",
    "\n",
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81618675",
   "metadata": {},
   "outputs": [],
   "source": [
    "## create the list for store the data\n",
    "\n",
    "Rank = []\n",
    "Name = []\n",
    "Net_Worth= []\n",
    "Age = []\n",
    "Citizenship = []\n",
    "Source = []\n",
    "Industry = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8d29ebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "## run a for loop for scraping the Rank data\n",
    "\n",
    "rank=driver.find_elements(By.XPATH,\"//div[@class='Table_rank__X4MKf']/div\")\n",
    "for i in rank:\n",
    "    if i.text is None :\n",
    "        Rank.append(\"--\") \n",
    "    else:\n",
    "        Rank.append(i.text)\n",
    "print(len(Rank),Rank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a7b9722",
   "metadata": {},
   "outputs": [],
   "source": [
    "## run a for loop for scraping the Name data\n",
    "\n",
    "name=driver.find_elements(By.XPATH,\"//div[@class='Table_personName__Bus2E']\")\n",
    "for i in name:\n",
    "    if i.text is None :\n",
    "        Name.append(\"--\") \n",
    "    else:\n",
    "        Name.append(i.text)\n",
    "print(len(Name),Name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa94b783",
   "metadata": {},
   "outputs": [],
   "source": [
    "## run a for loop for scraping the Net worth data\n",
    "\n",
    "net_worth=driver.find_elements(By.XPATH,\"//div[@class='Table_finalWorth__UZA6k']/span\")\n",
    "for i in net_worth:\n",
    "    if i.text is None :\n",
    "        Net_Worth.append(\"--\") \n",
    "    else:\n",
    "        Net_Worth.append(i.text)\n",
    "print(len(Net_Worth),Net_Worth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94d45908",
   "metadata": {},
   "outputs": [],
   "source": [
    "## run a for loop for scraping the Age data\n",
    "\n",
    "age=driver.find_elements(By.XPATH,\"//div[@class='Table_tableRow__lF_cY']/div[4]\")\n",
    "for i in age:\n",
    "    if i.text is None :\n",
    "        Age.append(\"--\") \n",
    "    else:\n",
    "        Age.append(i.text)\n",
    "print(len(Age),Age)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41af99ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "## run a for loop for scraping the Citizenship data\n",
    "\n",
    "citizenship=driver.find_elements(By.XPATH,\"//div[@class='Table_tableRow__lF_cY']/div[5]\")\n",
    "for i in citizenship:\n",
    "    if i.text is None :\n",
    "        Citizenship.append(\"none\") \n",
    "    else:\n",
    "        Citizenship.append(i.text)\n",
    "print(len(Citizenship),Citizenship)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9267c2f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## run a for loop for scraping the Industry data\n",
    "\n",
    "industry=driver.find_elements(By.XPATH,\"//div[@class='Table_tableRow__lF_cY']/div[6]\")\n",
    "for i in industry:\n",
    "    if i.text is None :\n",
    "        Industry.append(\"none\") \n",
    "    else:\n",
    "        Industry.append(i.text)\n",
    "print(len(Industry),Industry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb642289",
   "metadata": {},
   "outputs": [],
   "source": [
    "## run a for loop for scraping the Source of wealth data\n",
    "\n",
    "source=driver.find_elements(By.XPATH,\"//div[@class='Table_tableRow__lF_cY']/div[7]\")\n",
    "for i in source:\n",
    "    if i.text is None :\n",
    "        Source.append(\"none\") \n",
    "    else:\n",
    "        Source.append(i.text)\n",
    "print(len(Source),Source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "212f8be9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating dataframe\n",
    "\n",
    "forbes_list=pd.DataFrame([])\n",
    "forbes_list['Rank']=Rank[0:100]\n",
    "forbes_list['Name']=Name[0:100]\n",
    "forbes_list['Net Worth']=Net_Worth[0:100]\n",
    "forbes_list['Age']=Age[0:100]\n",
    "forbes_list['Citizenship']=Citizenship[0:100]\n",
    "forbes_list['Source']=Source[0:100]\n",
    "forbes_list['Industry']=Industry[0:100]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08905f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "forbes_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a0e0c55",
   "metadata": {},
   "source": [
    "### 8. Write a program to extract at least 500 Comments, Comment upvote and time when comment was posted from any YouTube Video."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0a35c90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connecting to webdriver\n",
    "driver=webdriver.Chrome()\n",
    "time.sleep(1)\n",
    "\n",
    "# Opening url in chrome browser\n",
    "url=\"https://www.youtube.com\"\n",
    "driver.get(url)\n",
    "time.sleep(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b5412d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding search menu by xpath\n",
    "Search=driver.find_element(By.XPATH,'//input[@id=\"search\"]') \n",
    "\n",
    "# Feeding input video name by user to search menu through send keys\n",
    "Search.send_keys('What are the MOST LIKED Youtube Comments on Youtube?')\n",
    "\n",
    "# Finding Search button for clicking through xpath\n",
    "Search_button=driver.find_element(By.XPATH,'//button[@id=\"search-icon-legacy\"]/yt-icon')  \n",
    "\n",
    "# Clicking search button\n",
    "Search_button.click()\n",
    "time.sleep(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3015c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clicking on video\n",
    "link = driver.find_element(By.XPATH,\"//yt-formatted-string[@class ='style-scope ytd-video-renderer']\")\n",
    "link.click()\n",
    "time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aa584a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scrolling window using ScrollBy method from 0 pixel to 1500 pixel\n",
    "driver.execute_script(\"window.scrollBy(0,100)\",\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f2c672a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the lists for store the data\n",
    "Comments = []\n",
    "Comment_posted_ago = []\n",
    "Timeline = []\n",
    "Likes = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81246232",
   "metadata": {},
   "outputs": [],
   "source": [
    "#extracting comment on video\n",
    "\n",
    "comment = driver.find_elements(By.ID,\"content-text\")\n",
    "time.sleep(1)\n",
    "for i in tqdm(comment):\n",
    "    if i.text is None:\n",
    "        Comments.append(\"--\")\n",
    "    else:\n",
    "        Comments.append(i.text)\n",
    "time.sleep(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1464616c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting time of commenting\n",
    "\n",
    "timeline = driver.find_elements(By.XPATH,\"//a[contains(text(),'ago')]\")\n",
    "time.sleep(3)\n",
    "for i in tqdm(timeline):\n",
    "    if i.text is None:\n",
    "        Timeline.append(\"-\")\n",
    "    else:\n",
    "        Timeline.append(i.text)\n",
    "for i in tqdm(range(0,len(Timeline),2)):\n",
    "    Comment_posted_ago.append(Timeline[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44b495f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## verify the length of comments \n",
    "\n",
    "len(Comments),len(Comment_posted_ago)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ecc7e21",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create the dataframe \n",
    "\n",
    "you_tube=pd.DataFrame({})\n",
    "you_tube['Comments']=Comments[:500]\n",
    "you_tube['Comment Posted Ago']=Comment_posted_ago[:500]\n",
    "\n",
    "you_tube\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdc54cf7",
   "metadata": {},
   "source": [
    "### 9. Write a python program to scrape a data for all available Hostels from `https://www.hostelworld.com/` in “London” location. You have to scrape hostel name, distance from city centre, ratings, total reviews, overall reviews, privates from price, dorms from price, facilities and property description."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "504ec3b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connecting to webdriver\n",
    "driver=webdriver.Chrome()\n",
    "time.sleep(1)\n",
    "\n",
    "# Opening url in chrome browser\n",
    "url=\"https://www.hostelworld.com\"\n",
    "driver.get(url)\n",
    "time.sleep(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4691ea16",
   "metadata": {},
   "outputs": [],
   "source": [
    "## place input in the searchh bar\n",
    "\n",
    "loc=driver.find_element(By.XPATH,'/html/body/div[3]/div/div[2]/main/header/div/div[2]/div[1]/div[1]/div/div[1]/div[1]/div/div[2]/label/input')\n",
    "loc.send_keys(\"london\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "459bbd59",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Click on the search bar for finalize the location\n",
    "\n",
    "location_search = driver.find_element(By.XPATH,\"//li[@class='item is-two-row']/button\")\n",
    "location_search.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9332f23",
   "metadata": {},
   "outputs": [],
   "source": [
    "## now click on the search button\n",
    "\n",
    "search_button = driver.find_element(By.XPATH,\"//button[@class='large-button btn-content']\") \n",
    "search_button.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6636ef3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create the lsit for store the data\n",
    "\n",
    "Hostel_name=[]\n",
    "Distance_from_city_centre=[]\n",
    "Ratings=[]\n",
    "Total_reviews=[]\n",
    "Overall_reviews=[]\n",
    "Privates_from_price=[]\n",
    "Dorms_from_price=[]\n",
    "Product_url = []\n",
    "\n",
    "# not able to find these two elements data on the same page \n",
    "\n",
    "# Facilities=[]\n",
    "# Property_discription=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9364af42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scrape the data using Attibute types and store in the list\n",
    "# Names\n",
    "\n",
    "names=driver.find_elements(By.XPATH,\"//div[@class='property-name']/span\")\n",
    "for i in names:\n",
    "    if i.text is None :\n",
    "        Hostel_name.append(\"--\") \n",
    "    else:\n",
    "        Hostel_name.append(i.text)\n",
    "print(len(Hostel_name),Hostel_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c7615d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Distance from the city centre\n",
    "\n",
    "distances=driver.find_elements(By.XPATH,\"//span[@class='distance-description']\")\n",
    "for i in distances:\n",
    "    if i.text is None :\n",
    "        Distance_from_city_centre.append(\"--\") \n",
    "    else:\n",
    "        Distance_from_city_centre.append(i.text)\n",
    "print(len(Distance_from_city_centre),Distance_from_city_centre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24ad42e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Ratings\n",
    "\n",
    "ratings=driver.find_elements(By.XPATH,\"//span[@class='number']\")\n",
    "for i in ratings:\n",
    "    if i.text is None :\n",
    "        Ratings.append(\"--\") \n",
    "    else:\n",
    "        Ratings.append(i.text)\n",
    "print(len(Ratings),Ratings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf1ad08d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Total Review Count\n",
    "\n",
    "total_reviews=driver.find_elements(By.XPATH,\"//div[@class='review']/span\")\n",
    "for i in total_reviews:\n",
    "    if i.text is None :\n",
    "        Total_reviews.append(\"--\") \n",
    "    else:\n",
    "        Total_reviews.append(i.text)\n",
    "print(len(Total_reviews),Total_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28d9aba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Overall review\n",
    "\n",
    "overall_revivews=driver.find_elements(By.XPATH,\"//span[@class='keyword']\")\n",
    "for i in overall_revivews:\n",
    "    if i.text is None :\n",
    "        Overall_reviews.append(\"--\") \n",
    "    else:\n",
    "        Overall_reviews.append(i.text)\n",
    "print(len(Overall_reviews),Overall_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a90f5ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "## private type rent \n",
    "\n",
    "private_prices=driver.find_elements(By.TAG_NAME,\"strong\")\n",
    "for i in private_prices:\n",
    "    if i.text is None :\n",
    "        Privates_from_price.append(\"--\") \n",
    "    else:\n",
    "        Privates_from_price.append(i.text)\n",
    "print(len(Privates_from_price),Privates_from_price)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64b4bae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Dormatory type rent \n",
    "\n",
    "dorms_prices=driver.find_elements(By.CLASS_NAME,\"current\")\n",
    "for i in dorms_prices:\n",
    "    if i.text is None :\n",
    "        Dorms_from_price.append(\"--\") \n",
    "    else:\n",
    "        Dorms_from_price.append(i.text)\n",
    "print(len(Dorms_from_price),Dorms_from_price)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f074915",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Property URL\n",
    "\n",
    "product_urls = driver.find_elements(By.XPATH,\"//div[@class='property-info-container']\")\n",
    "for i in product_urls:\n",
    "    if i.text is None :\n",
    "        Product_url.append(\"--\") \n",
    "    else:\n",
    "        Product_url.append(i.text)\n",
    "print(len(Product_url),Product_url)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba78b71c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create the DataFrame \n",
    "\n",
    "hostels=pd.DataFrame([])\n",
    "hostels['Hostel_name']=Hostel_name[0:30]\n",
    "hostels['Distance_from_city_centre']=Distance_from_city_centre[0:30]\n",
    "hostels['Ratings']=Ratings[0:30]\n",
    "hostels['Total_reviews']=Total_reviews[0:30]\n",
    "hostels['Overall_reviews']=Overall_reviews[0:30]\n",
    "hostels['Privates_from_price']=Privates_from_price[0:30]\n",
    "hostels['Dorms_from_price']=Dorms_from_price[0:30]\n",
    "hostels['Product_url']=Product_url[0:30]\n",
    "\n",
    "#hostels['Facilities']=Facilities[0:30]\n",
    "#hostels['Property_discription']=Property_discription[0:30]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11362f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "hostels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f2ccb79",
   "metadata": {},
   "source": [
    "### End Of all Questions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
